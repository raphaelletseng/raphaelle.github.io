I"²<p>This is a short summary of <a href="https://www.aclweb.org/anthology/2020.acl-main.463/">Bender and Kollerâ€™s â€˜Climbing Towards NLU: On Meaning, Form, and
Understanding in the Age of Dataâ€™., 2020</a>.</p>

<p>This paper aims to argue that a system exposed only to form in its training has a priori no way to learn meaning. Language modelling
tasks cannot lead to the learning of meaning, as only form is used in the training data. The paper advocates for an alignment
between claims and methodology.</p>

<p>The hype around large LMs and the danger this may pose to the field is evidenced in the misleading language used in academicallyoriented publications, where gross overclaims of model capabilities, such as â€˜a model that understandsâ€™ and â€˜machine
comprehensionâ€™, are used. Such statements only serve to feed the AI hype in popular press and the use of imprecise language tends
to occur when we donâ€™t fully comprehend what large LMs implicitly represent about language. The paper argues that instead, a
greater effort should be made to explicitly define terms, in order to accurately represent the capabilities of NLP systems as the field
grows. I agree with this point as oftentimes, it seems that the media and the academic field create an echo chamber, using
sensationalist language to fuel the â€˜hypeâ€™ around the field. Mainstream media often fails to explain the workings and the research
behind these models, leaving the public forced to trust that these â€˜black-boxâ€™ methods work.
The terms â€˜meaningâ€™ and â€˜formâ€™, in the context of the paper, are then explicitly defined. â€˜Formâ€™ is taken to mean â€˜any observable
realisation of language, marks on a page, pixels or bytes, movement of the articulatorsâ€™. â€˜Meaningâ€™ is described as â€˜the relation
between form and something external to languageâ€™. Formally, meaning is represented by a pair (e, i) of natural language expressions
e and the communicative intent i they can evoke. From this, â€˜understandâ€™ can be defined as â€˜the process of retrieving i from a given
eâ€™. â€˜Standing meaningâ€™ s, or the â€˜meaning that remains constant across all of its possible contexts of useâ€™, allows us to formally
assume that when the listener hears an expression e, they can then reconstructs s and use their own knowledge of the
communicative situation and the speakerâ€™s state of mind and intention to deduce i. This definition differs from the one we saw on
distributional semantics, which was described as â€˜understanding a term by the distribution of words that appear near the termâ€™. This
definition doesnâ€™t seem to align with the thesis of this paper as it is the form of the text, and not â€˜something external to languageâ€™
that is used to claim the model â€˜understandsâ€™. From these definitions, the argument of the paper is expanded to state that if the
training data is only form, there isnâ€™t sufficient signal to learn the relation m (meaning) between said form and the non-linguistic
intent of the human language users, nor any relation between e (expression) and s (standing meaning), assigned by the linguistic
system to each form.</p>

<p>The paper presents several thought experiments to demonstrate its point, denoting â€˜intelligenceâ€™ as â€˜meaning and understandingâ€™.
From Searleâ€™s Chinese Room, it concludes that independent of whether passing the Turing test would mean a system is intelligent, a
system trained only on form would fail a sufficiently sensitive test, because it would lack the ability to connect its utterances to the
world. The Octopus Test and the example of asking GPT-2 â€˜Help!â€¦ What should I do?â€™ reinforce the point that language models fail
when we test their understanding of â€˜meaningâ€™ by asking them to solve tasks that require reasoning and creative thinking.
From observing human children, the paper argues that passive exposure isnâ€™t sufficient to acquire a linguistic system; joint attention,
an awareness of what another person is attending to, and an idea of what theyâ€™re intending to communicate are all required.
However, it recognises that grounding distributional representations in the real world is challenging and proposes tackling these
issues by training distributional models on corpora augmented with perceptual data or looking to interaction data.</p>

<p>The paper brings into question whether the field is tackling the correct problems. It pushes for a â€˜top-downâ€™ approach, where focus
is kept on the overarching, end goal of finding a unified theory for the whole field. Whilst a â€˜bottom-upâ€™ perspective may currently
be responsible for the hype, excitement, and general optimism in the field, it fails to address the question of whether the â€˜right hill is
being climbedâ€™. Given that we will only be able to tell in hindsight, the paper proposes five ways of mitigating error: cultivating
humility and asking top down questions, being aware of the limitations of tasks, valuing and supporting the work of carefully
creating new tasks, evaluating models of meaning across tasks, and performing thorough analysis of both error and successes. It
addresses prevalent counterarguments against the main thesis succinctly and effectively, before concluding that, in contrast to the
current hype, meaning cannot be learnt from form alone. Large LMs do not learn meaning. It finishes by offering thoughts on how to
maintain healthy optimism in the field. Calls are made to spur a top-down perspective in the field, to increase the use of precise
language, especially when talking about the successes of current models, and to encourage humility in dealing with natural
language. I inherently agree with the thesis and the conclusions of this paper â€“ the methods to mitigate error could be generalised
to other fields where a lot of hype has recently been generated too. The use of examples via thought experiments and the answers
to the counterarguments make for a solid argument, advocating for chartering a more attentive and vigilant course in the field.</p>
:ET